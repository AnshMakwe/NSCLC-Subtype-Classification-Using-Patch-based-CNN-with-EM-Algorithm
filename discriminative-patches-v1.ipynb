{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8521041,"sourceType":"datasetVersion","datasetId":5087572}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nimport cv2\nfrom tqdm import tqdm\nimport random\nfrom scipy.ndimage import gaussian_filter\nimport pydicom\nimport numpy as np\nfrom skimage import exposure\nfrom skimage.transform import resize\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:16.509329Z","iopub.execute_input":"2025-05-04T05:11:16.509890Z","iopub.status.idle":"2025-05-04T05:11:30.886199Z","shell.execute_reply.started":"2025-05-04T05:11:16.509866Z","shell.execute_reply":"2025-05-04T05:11:30.885443Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class NSCLCPatchDataset(Dataset):\n    def __init__(self, root_dir, patient_ids, magnification='20X', transform=None, patch_size=500, \n                 sub_patch_size=400, augment=True):\n        self.root_dir = root_dir\n        self.patient_ids = patient_ids\n        self.magnification = magnification\n        self.transform = transform\n        self.patch_size = patch_size\n        self.sub_patch_size = sub_patch_size\n        self.augment = augment\n        \n        # For NSCLC dataset, we have two classes: ADC (0) and SCC (1)\n        # We'll need to determine this from the metadata or a separate file\n        self.class_mapping = {\n            'ADC': 0,\n            'SCC': 1\n        }\n        \n        # Load patient data and labels\n        self.patches = []\n        self.labels = []\n        self.image_indices = []  # To track which image each patch belongs to\n        \n        self._load_patches()\n    \n    def _load_patches(self):\n        print(f\"Loading patches for {len(self.patient_ids)} patients...\")\n        \n        for i, patient_id in enumerate(tqdm(self.patient_ids)):\n            patient_dir = os.path.join(self.root_dir, patient_id)\n            \n            # Find the study folder (there's usually one per patient)\n            study_folders = [f for f in os.listdir(patient_dir) if os.path.isdir(os.path.join(patient_dir, f))]\n            \n            if not study_folders:\n                continue\n                \n            study_dir = os.path.join(patient_dir, study_folders[0])\n            \n            # Find the folder with CT images (usually the one with many .dcm files)\n            ct_folders = [f for f in os.listdir(study_dir) if os.path.isdir(os.path.join(study_dir, f))]\n            \n            if not ct_folders:\n                continue\n                \n            # Typically the first folder contains the CT images\n            ct_dir = os.path.join(study_dir, ct_folders[0])\n            \n            # Get all DICOM files\n            dcm_files = [f for f in os.listdir(ct_dir) if f.endswith('.dcm')]\n            \n            if not dcm_files:\n                continue\n                \n            # Load DICOM files and extract patches\n            # For simplicity, we'll use a subset of slices\n            slice_step = max(1, len(dcm_files) // 20)  # Take about 20 slices per patient\n            \n            # Determine patient label (ADC or SCC)\n            # This would typically come from clinical data\n            # For now, we'll use a placeholder approach\n            if 'ADC' in patient_id:\n                label = self.class_mapping['ADC']\n            elif 'SCC' in patient_id:\n                label = self.class_mapping['SCC']\n            else:\n                # If label can't be determined, assign randomly for now\n                # In a real implementation, you'd want to read this from metadata\n                label = random.randint(0, 1)\n            \n            for j, dcm_file in enumerate(sorted(dcm_files)[::slice_step]):\n                try:\n                    # Load DICOM file\n                    ds = pydicom.dcmread(os.path.join(ct_dir, dcm_file))\n                    if 'PixelData' not in ds:\n                        continue \n                    \n                    # Convert to numpy array\n                    img = ds.pixel_array\n                    \n                    # Handle different image dimensions\n                    if len(img.shape) > 2:\n                        # For 3D or 4D images, handle differently\n                        if len(img.shape) == 3:\n                            img = img[img.shape[0]//2]  # Take middle slice\n                        # If it's RGB or has channels\n                        elif len(img.shape) == 3 and img.shape[2] <= 4:\n                            img = exposure.rescale_intensity(img, out_range=(0, 255)).astype(np.uint8)\n                        else:\n                            # Skip images with unexpected dimensions\n                            continue\n                    else:\n                        # For 2D grayscale images\n                        img = exposure.rescale_intensity(img, out_range=(0, 255)).astype(np.uint8)\n                    \n                    # Now check if image is large enough for patch extraction\n                    if img.shape[0] < self.patch_size or img.shape[1] < self.patch_size:\n                        # Resize if needed - make sure to specify correct output shape\n                        new_size = (self.patch_size, self.patch_size, 3)  # Include channel dimension\n                        img = resize(img, new_size, preserve_range=True).astype(np.uint8)\n                    \n                    # Extract patches from this slice\n                    num_patches_per_slice = 5\n                    \n                    for k in range(num_patches_per_slice):\n                        # Check if image is large enough for patch extraction\n                        if img.shape[0] < self.patch_size or img.shape[1] < self.patch_size:\n                            # Resize if needed\n                            scale = max(self.patch_size / img.shape[0], self.patch_size / img.shape[1])\n                            new_size = (int(img.shape[0] * scale), int(img.shape[1] * scale), 3)  # Include channel dimension\n                            img = resize(img, new_size, preserve_range=True).astype(np.uint8)\n                        \n                        # Random crop to extract patch\n                        if img.shape[0] > self.patch_size and img.shape[1] > self.patch_size:\n                            x = random.randint(0, img.shape[1] - self.patch_size)\n                            y = random.randint(0, img.shape[0] - self.patch_size)\n                            patch = img[y:y+self.patch_size, x:x+self.patch_size]\n                        else:\n                            # If image is too small, pad it\n                            patch = np.zeros((self.patch_size, self.patch_size, 3), dtype=np.uint8)\n                            patch[:min(img.shape[0], self.patch_size), :min(img.shape[1], self.patch_size)] = img[:min(img.shape[0], self.patch_size), :min(img.shape[1], self.patch_size)]\n                        \n                        # Check if patch has enough tissue (simplified)\n                        # In a real implementation, you'd want a more sophisticated method\n                        if np.mean(patch) > 10:  # Simple threshold to check if patch has content\n                            self.patches.append(patch)\n                            self.labels.append(label)\n                            self.image_indices.append(i)\n                \n                except Exception as e:\n                    print(f\"Error processing {dcm_file}: {e}\")\n    \n    def __len__(self):\n        return len(self.patches)\n    \n    def __getitem__(self, idx):\n        patch = self.patches[idx]\n        label = self.labels[idx]\n        image_idx = self.image_indices[idx]\n        \n        # Convert to PIL Image for transformations\n        patch = Image.fromarray(patch)\n        \n        # Ensure image is in RGB mode\n        if patch.mode != 'RGB':\n            patch = patch.convert('RGB')\n        \n        if self.augment:\n            # 1. Random crop (sub-patch selection)\n            i, j = random.randint(0, self.patch_size - self.sub_patch_size), random.randint(0, self.patch_size - self.sub_patch_size)\n            patch = transforms.functional.crop(patch, i, j, self.sub_patch_size, self.sub_patch_size)\n            \n            # 2. Random rotation and mirroring\n            if random.random() > 0.5:\n                patch = transforms.functional.hflip(patch)\n            rotation_angle = random.choice([0, 90, 180, 270])\n            patch = transforms.functional.rotate(patch, rotation_angle)\n        \n        if self.transform:\n            patch = self.transform(patch)\n            \n        return patch, label, image_idx\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:30.887520Z","iopub.execute_input":"2025-05-04T05:11:30.888025Z","iopub.status.idle":"2025-05-04T05:11:30.904632Z","shell.execute_reply.started":"2025-05-04T05:11:30.888006Z","shell.execute_reply":"2025-05-04T05:11:30.904086Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class PatchCNN(nn.Module):\n    def __init__(self, num_classes=2):\n        super(PatchCNN, self).__init__()\n        \n        # First convolutional block\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=9, stride=3, padding=0)\n        self.bn1 = nn.BatchNorm2d(96)  # Using BatchNorm instead of LRN\n        self.relu1 = nn.ReLU(inplace=True)\n        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Second convolutional block\n        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(256)  # Using BatchNorm instead of LRN\n        self.relu2 = nn.ReLU(inplace=True)\n        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        # Third convolutional block\n        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n        self.relu3 = nn.ReLU(inplace=True)\n        \n        # Fourth convolutional block\n        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n        self.relu4 = nn.ReLU(inplace=True)\n        \n        # Fifth convolutional block\n        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n        self.relu5 = nn.ReLU(inplace=True)\n        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((15, 15))\n        \n        # Calculate the output size of the last convolutional layer\n        with torch.no_grad():\n            dummy_input = torch.zeros(1, 3, 400, 400)\n            x = self.pool1(self.relu1(self.bn1(self.conv1(dummy_input))))\n            x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n            x = self.relu3(self.conv3(x))\n            x = self.relu4(self.conv4(x))\n            x = self.pool5(self.relu5(self.conv5(x)))\n            x = self.adaptive_pool(x)\n            fc_input_size = x.view(1, -1).size(1)\n            print(f\"Feature map size after convolutions: {fc_input_size}\")\n        \n        # Create classifier with dynamically calculated input size\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(fc_input_size, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes)\n        )\n        \n    def forward(self, x):\n        # Feature extraction\n        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n        x = self.relu3(self.conv3(x))\n        x = self.relu4(self.conv4(x))\n        x = self.pool5(self.relu5(self.conv5(x)))\n\n        x = self.adaptive_pool(x)\n        \n        # Flatten\n        x = x.view(x.size(0), -1)\n        \n        # Classification\n        x = self.classifier(x)\n        \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:31.048277Z","iopub.execute_input":"2025-05-04T05:11:31.048559Z","iopub.status.idle":"2025-05-04T05:11:31.058293Z","shell.execute_reply.started":"2025-05-04T05:11:31.048534Z","shell.execute_reply":"2025-05-04T05:11:31.057796Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class EMBasedPatchSelection:\n    def __init__(self, model_20x, model_5x, p1=30, p2=50):\n        \"\"\"\n        Args:\n            model_20x: CNN model trained on 20X magnification\n            model_5x: CNN model trained on 5X magnification\n            p1: P1-th percentile for image-level threshold (default: 30)\n            p2: P2-th percentile for class-level threshold (default: 50)\n        \"\"\"\n        self.model_20x = model_20x\n        self.model_5x = model_5x\n        self.p1 = p1\n        self.p2 = p2\n        \n    def compute_patch_probabilities(self, dataloader):\n        \"\"\"\n        Compute P(y_i|x_ij) for all patches by averaging predictions from two CNNs\n        \"\"\"\n        self.model_20x.eval()\n        self.model_5x.eval()\n        \n        patch_probs = []\n        image_indices = []\n        labels = []\n        \n        with torch.no_grad():\n            for patches, batch_labels, batch_image_indices in dataloader:\n                # Get predictions from both models\n                patches = patches.cuda()\n                outputs_20x = torch.softmax(self.model_20x(patches), dim=1)\n                outputs_5x = torch.softmax(self.model_5x(patches), dim=1)\n                \n                # Average predictions\n                avg_outputs = (outputs_20x + outputs_5x) / 2\n                \n                # Extract probability for the true class\n                batch_probs = torch.gather(avg_outputs, 1, batch_labels.unsqueeze(1).cuda()).cpu().numpy()\n                \n                patch_probs.extend(batch_probs)\n                image_indices.extend(batch_image_indices.numpy())\n                labels.extend(batch_labels.numpy())\n        \n        return np.array(patch_probs).flatten(), np.array(image_indices), np.array(labels)\n    \n    def apply_gaussian_smoothing(self, patch_probs, image_indices, sigma=1.0):\n        \"\"\"\n        Apply Gaussian smoothing to probability maps to compute P(H_ij|X)\n        \"\"\"\n        # Group probabilities by image\n        unique_images = np.unique(image_indices)\n        smoothed_probs = np.copy(patch_probs)\n        \n        # In a real implementation, you would need spatial information about patches\n        # For simplicity, we'll just apply Gaussian smoothing to the 1D array of probabilities per image\n        for img_idx in unique_images:\n            img_mask = (image_indices == img_idx)\n            img_probs = patch_probs[img_mask]\n            \n            # Apply Gaussian smoothing\n            smoothed_img_probs = gaussian_filter(img_probs, sigma=sigma)\n            \n            # Update the smoothed probabilities\n            smoothed_probs[img_mask] = smoothed_img_probs\n        \n        return smoothed_probs\n    \n    def select_discriminative_patches(self, smoothed_probs, image_indices, labels):\n        \"\"\"\n        Select discriminative patches based on thresholds\n        \"\"\"\n        unique_images = np.unique(image_indices)\n        unique_classes = np.unique(labels)\n        \n        # Initialize mask for discriminative patches\n        is_discriminative = np.zeros_like(smoothed_probs, dtype=bool)\n        \n        # Compute thresholds and select discriminative patches\n        for img_idx in unique_images:\n            img_mask = (image_indices == img_idx)\n            img_probs = smoothed_probs[img_mask]\n            \n            # Image-level threshold (Hi): P1-th percentile of Si\n            img_threshold = np.percentile(img_probs, self.p1)\n            \n            # Get class of this image (assuming all patches from same image have same label)\n            img_class = labels[img_mask][0]\n            \n            # Class-level threshold (Ri): P2-th percentile of Ec\n            class_mask = (labels == img_class)\n            class_probs = smoothed_probs[class_mask]\n            class_threshold = np.percentile(class_probs, self.p2)\n            \n            # Final threshold (Tij): min(Hi, Ri)\n            threshold = min(img_threshold, class_threshold)\n            \n            # Select discriminative patches\n            is_discriminative[img_mask] = (img_probs >= threshold)\n        \n        return is_discriminative\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:37.561758Z","iopub.execute_input":"2025-05-04T05:11:37.562228Z","iopub.status.idle":"2025-05-04T05:11:37.573971Z","shell.execute_reply.started":"2025-05-04T05:11:37.562196Z","shell.execute_reply":"2025-05-04T05:11:37.573077Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class DecisionFusionModel:\n    def __init__(self, classifier_type='logistic_regression'):\n        \"\"\"\n        Args:\n            classifier_type: Type of classifier to use ('logistic_regression' or 'svm')\n        \"\"\"\n        self.classifier_type = classifier_type\n        \n        if classifier_type == 'logistic_regression':\n            self.classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n        elif classifier_type == 'svm':\n            self.classifier = SVC(kernel='rbf', probability=True)\n        else:\n            raise ValueError(\"classifier_type must be 'logistic_regression' or 'svm'\")\n    \n    def create_histograms(self, models, dataloader, device):\n        \"\"\"\n        Create histograms of patch-level predictions for each image\n        \n        Args:\n            models: List of CNN models\n            dataloader: DataLoader for patches\n            device: Device to run inference on\n        \n        Returns:\n            histograms: Histograms of patch-level predictions for each image\n            image_labels: Labels for each image\n        \"\"\"\n        # Set all models to evaluation mode\n        for model in models:\n            model.eval()\n        \n        # Get number of classes from the first model\n        num_classes = models[0].classifier[-1].out_features\n        \n        # Initialize dictionaries to store predictions and counts\n        image_predictions = {}\n        image_labels = {}\n        \n        with torch.no_grad():\n            for patches, batch_labels, batch_image_indices in dataloader:\n                patches = patches.to(device)\n                \n                # Get predictions from all models and concatenate\n                all_model_probs = []\n                \n                for model in models:\n                    outputs = model(patches)\n                    probs = torch.softmax(outputs, dim=1).cpu().numpy()\n                    all_model_probs.append(probs)\n                \n                # Concatenate probabilities from all models\n                combined_probs = np.concatenate(all_model_probs, axis=1)\n                \n                # Update image predictions and labels\n                for i, img_idx in enumerate(batch_image_indices.numpy()):\n                    if img_idx not in image_predictions:\n                        image_predictions[img_idx] = np.zeros(combined_probs.shape[1])\n                        image_labels[img_idx] = batch_labels[i].item()\n                    \n                    # Sum up probabilities for histogram\n                    image_predictions[img_idx] += combined_probs[i]\n        \n        # Convert dictionaries to arrays\n        unique_images = sorted(image_predictions.keys())\n        histograms = np.array([image_predictions[img_idx] for img_idx in unique_images])\n        labels = np.array([image_labels[img_idx] for img_idx in unique_images])\n        \n        return histograms, labels\n\n\n    \n    def fit(self, histograms, labels):\n        \"\"\"\n        Fit the decision fusion model\n        \"\"\"\n        self.classifier.fit(histograms, labels)\n    \n    def predict(self, histograms):\n        \"\"\"\n        Predict image-level labels\n        \"\"\"\n        return self.classifier.predict(histograms)\n    \n    def predict_proba(self, histograms):\n        \"\"\"\n        Predict image-level class probabilities\n        \"\"\"\n        return self.classifier.predict_proba(histograms)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:44.924592Z","iopub.execute_input":"2025-05-04T05:11:44.925248Z","iopub.status.idle":"2025-05-04T05:11:44.933665Z","shell.execute_reply.started":"2025-05-04T05:11:44.925226Z","shell.execute_reply":"2025-05-04T05:11:44.933115Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train_patch_cnn(model, train_loader, val_loader, num_epochs=10):\n    \"\"\"\n    Train a patch-level CNN model\n    \"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n    \n    model = model.cuda()\n    best_val_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for patches, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n            patches, labels = patches.cuda(), labels.cuda()\n            \n            optimizer.zero_grad()\n            \n            outputs = model(patches)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        \n        with torch.no_grad():\n            for patches, labels, _ in val_loader:\n                patches, labels = patches.cuda(), labels.cuda()\n                \n                outputs = model(patches)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                \n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss / len(val_loader)\n        val_acc = 100. * val_correct / val_total\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        \n        # Save best model\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_patch_cnn.pth')\n        \n        scheduler.step()\n    \n    return model\n\ndef train_em_based_model(dataset, num_iterations=3):\n    \"\"\"\n    Train the full EM-based model with discriminative patch selection\n    \"\"\"\n    # Split dataset into train and validation\n    train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, stratify=dataset.labels)\n    \n    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    # Initialize models for 20X and 5X magnifications\n    model_20x = PatchCNN(num_classes=2)  # Assuming binary classification (ADC vs SCC)\n    model_5x = PatchCNN(num_classes=2)\n    \n    # Initial training with all patches\n    print(\"Initial training with all patches...\")\n    model_20x = train_patch_cnn(model_20x, train_loader, val_loader, num_epochs=5)\n    model_5x = train_patch_cnn(model_5x, train_loader, val_loader, num_epochs=5)\n    \n    # Initialize EM-based patch selection\n    em_selector = EMBasedPatchSelection(model_20x, model_5x)\n    \n    # EM iterations\n    for iteration in range(num_iterations):\n        print(f\"\\nEM Iteration {iteration+1}/{num_iterations}\")\n        \n        # E-step: Select discriminative patches\n        print(\"E-step: Selecting discriminative patches...\")\n        patch_probs, image_indices, labels = em_selector.compute_patch_probabilities(val_loader)\n        smoothed_probs = em_selector.apply_gaussian_smoothing(patch_probs, image_indices)\n        is_discriminative = em_selector.select_discriminative_patches(smoothed_probs, image_indices, labels)\n        \n        # Create a new dataset with only discriminative patches\n        discriminative_indices = [idx for idx, is_disc in enumerate(is_discriminative) if is_disc]\n        discriminative_train_dataset = torch.utils.data.Subset(train_dataset, discriminative_indices)\n        discriminative_train_loader = DataLoader(discriminative_train_dataset, batch_size=32, shuffle=True, num_workers=4)\n        \n        # M-step: Retrain models with discriminative patches\n        print(\"M-step: Retraining models with discriminative patches...\")\n        model_20x = train_patch_cnn(model_20x, discriminative_train_loader, val_loader, num_epochs=5)\n        model_5x = train_patch_cnn(model_5x, discriminative_train_loader, val_loader, num_epochs=5)\n    \n    # Train additional models with different numbers of iterations\n    model_20x_early = PatchCNN(num_classes=2)\n    model_5x_early = PatchCNN(num_classes=2)\n    \n    model_20x_early.load_state_dict(torch.load('best_patch_cnn.pth'))\n    model_5x_early.load_state_dict(torch.load('best_patch_cnn.pth'))\n    \n    # Return all models for decision fusion\n    return [model_20x, model_5x, model_20x_early, model_5x_early]\n\ndef train_decision_fusion(models, dataset, device, classifier_type='logistic_regression'):\n    \"\"\"\n    Train the image-level decision fusion model\n    \"\"\"\n    # Split dataset into train and validation\n    train_indices, val_indices = train_test_split(range(len(dataset)), test_size=0.2, stratify=dataset.labels)\n    \n    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n    val_dataset = torch.utils.data.Subset(dataset, val_indices)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    # Initialize decision fusion model\n    fusion_model = DecisionFusionModel(classifier_type=classifier_type)\n    \n    # Make sure all models are on the same device\n    for model in models:\n        model.to(device)\n    \n    # Create histograms for training\n    print(\"Creating histograms for training...\")\n    train_histograms, train_labels = fusion_model.create_histograms(models, train_loader, device)\n    \n    # Fit decision fusion model\n    print(\"Fitting decision fusion model...\")\n    fusion_model.fit(train_histograms, train_labels)\n    \n    # Evaluate on validation set\n    print(\"Evaluating on validation set...\")\n    val_histograms, val_labels = fusion_model.create_histograms(models, val_loader, device)\n    val_predictions = fusion_model.predict(val_histograms)\n    \n    # Calculate accuracy\n    accuracy = np.mean(val_predictions == val_labels)\n    print(f\"Validation accuracy: {accuracy:.4f}\")\n    \n    return fusion_model\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:50.694973Z","iopub.execute_input":"2025-05-04T05:11:50.695509Z","iopub.status.idle":"2025-05-04T05:11:50.710296Z","shell.execute_reply.started":"2025-05-04T05:11:50.695488Z","shell.execute_reply":"2025-05-04T05:11:50.709687Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def main():\n    # Set random seed for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    \n    # Dataset path\n    dataset_path = '/kaggle/input/nsclc-radiomics/NSCLC-Radiomics'\n    \n    # Get list of patient IDs\n    patient_ids = [f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))]\n    \n    # Split patients into train and test sets (80% train, 20% test)\n    train_patients, test_patients = train_test_split(patient_ids, test_size=0.2, random_state=42)\n    \n    # Define transformations\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets for 20X and 5X magnifications\n    dataset_20x = NSCLCPatchDataset(\n        root_dir=dataset_path,\n        patient_ids=train_patients,\n        magnification='20X',\n        transform=transform,\n        patch_size=500,\n        sub_patch_size=400,\n        augment=True\n    )\n    \n    dataset_5x = NSCLCPatchDataset(\n        root_dir=dataset_path,\n        patient_ids=train_patients,\n        magnification='5X',\n        transform=transform,\n        patch_size=500,\n        sub_patch_size=400,\n        augment=True\n    )\n    \n    # Train EM-based models\n    print(\"Training EM-based models for 20X magnification...\")\n    models_20x = train_em_based_model(dataset_20x, num_iterations=3)\n    \n    print(\"Training EM-based models for 5X magnification...\")\n    models_5x = train_em_based_model(dataset_5x, num_iterations=3)\n    \n    # Combine all models\n    all_models = models_20x + models_5x\n    \n    # Train decision fusion model\n    print(\"Training decision fusion model...\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    fusion_model = train_decision_fusion(all_models, dataset_20x, device, classifier_type='logistic_regression')\n    \n    # Create test dataset\n    test_dataset = NSCLCPatchDataset(\n        root_dir=dataset_path,\n        patient_ids=test_patients,\n        magnification='20X',\n        transform=transform,\n        patch_size=500,\n        sub_patch_size=400,\n        augment=False\n    )\n    \n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n    \n    # Evaluate on test set\n    print(\"Evaluating on test set...\")\n    test_histograms, test_labels = fusion_model.create_histograms(all_models, test_loader, device)\n    test_predictions = fusion_model.predict(test_histograms)\n    \n    # Calculate accuracy\n    accuracy = np.mean(test_predictions == test_labels)\n    print(f\"Test accuracy: {accuracy:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T05:11:57.569444Z","iopub.execute_input":"2025-05-04T05:11:57.570010Z","iopub.status.idle":"2025-05-04T06:12:53.820672Z","shell.execute_reply.started":"2025-05-04T05:11:57.569988Z","shell.execute_reply":"2025-05-04T06:12:53.819714Z"}},"outputs":[{"name":"stdout","text":"Loading patches for 337 patients...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 337/337 [01:50<00:00,  3.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loading patches for 337 patients...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 337/337 [01:01<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training EM-based models for 20X magnification...\nFeature map size after convolutions: 57600\nFeature map size after convolutions: 57600\nInitial training with all patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 292/292 [01:11<00:00,  4.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6817, Train Acc: 57.91%, Val Loss: 0.6797, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 292/292 [01:13<00:00,  3.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6791, Train Acc: 57.91%, Val Loss: 0.6787, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 292/292 [01:14<00:00,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6780, Train Acc: 57.91%, Val Loss: 0.6774, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 292/292 [01:14<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6750, Train Acc: 57.90%, Val Loss: 0.6772, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 292/292 [01:14<00:00,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6716, Train Acc: 57.97%, Val Loss: 0.6696, Val Acc: 57.31%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 292/292 [01:14<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6818, Train Acc: 57.84%, Val Loss: 0.6806, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 292/292 [01:14<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6807, Train Acc: 57.91%, Val Loss: 0.6788, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 292/292 [01:14<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6779, Train Acc: 57.92%, Val Loss: 0.6795, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 292/292 [01:14<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6761, Train Acc: 57.80%, Val Loss: 0.6827, Val Acc: 56.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 292/292 [01:14<00:00,  3.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6739, Train Acc: 57.89%, Val Loss: 0.6749, Val Acc: 57.22%\n\nEM Iteration 1/3\nE-step: Selecting discriminative patches...\nM-step: Retraining models with discriminative patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 58/58 [00:15<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6654, Train Acc: 60.57%, Val Loss: 0.6746, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 58/58 [00:15<00:00,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6615, Train Acc: 60.84%, Val Loss: 0.6746, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 58/58 [00:15<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6598, Train Acc: 60.90%, Val Loss: 0.6745, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 58/58 [00:15<00:00,  3.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6548, Train Acc: 60.79%, Val Loss: 0.6746, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 58/58 [00:15<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6621, Train Acc: 60.57%, Val Loss: 0.6720, Val Acc: 57.95%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 58/58 [00:15<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6625, Train Acc: 61.00%, Val Loss: 0.6753, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 58/58 [00:15<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6634, Train Acc: 60.90%, Val Loss: 0.6741, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 58/58 [00:15<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6616, Train Acc: 60.90%, Val Loss: 0.6729, Val Acc: 59.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 58/58 [00:15<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6626, Train Acc: 61.28%, Val Loss: 0.6672, Val Acc: 58.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 58/58 [00:15<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6575, Train Acc: 61.00%, Val Loss: 0.6753, Val Acc: 57.99%\n\nEM Iteration 2/3\nE-step: Selecting discriminative patches...\nM-step: Retraining models with discriminative patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 57/57 [00:14<00:00,  3.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6644, Train Acc: 60.00%, Val Loss: 0.6704, Val Acc: 60.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 57/57 [00:14<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6615, Train Acc: 59.50%, Val Loss: 0.6736, Val Acc: 58.04%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 57/57 [00:15<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6551, Train Acc: 59.22%, Val Loss: 0.6778, Val Acc: 57.69%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 57/57 [00:14<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6585, Train Acc: 60.33%, Val Loss: 0.6667, Val Acc: 58.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 57/57 [00:14<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6600, Train Acc: 60.17%, Val Loss: 0.6698, Val Acc: 57.22%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 57/57 [00:14<00:00,  3.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6577, Train Acc: 60.22%, Val Loss: 0.6660, Val Acc: 58.81%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 57/57 [00:14<00:00,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6609, Train Acc: 59.55%, Val Loss: 0.6663, Val Acc: 57.44%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 57/57 [00:15<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6575, Train Acc: 60.72%, Val Loss: 0.6692, Val Acc: 58.34%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 57/57 [00:14<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6577, Train Acc: 60.33%, Val Loss: 0.6644, Val Acc: 59.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 57/57 [00:14<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6616, Train Acc: 59.89%, Val Loss: 0.6736, Val Acc: 57.22%\n\nEM Iteration 3/3\nE-step: Selecting discriminative patches...\nM-step: Retraining models with discriminative patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 58/58 [00:15<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6553, Train Acc: 59.57%, Val Loss: 0.6624, Val Acc: 61.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 58/58 [00:15<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6579, Train Acc: 59.25%, Val Loss: 0.6658, Val Acc: 58.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 58/58 [00:15<00:00,  3.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6599, Train Acc: 60.28%, Val Loss: 0.6652, Val Acc: 59.11%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 58/58 [00:15<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6599, Train Acc: 59.30%, Val Loss: 0.6668, Val Acc: 57.91%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 58/58 [00:14<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6553, Train Acc: 60.56%, Val Loss: 0.6694, Val Acc: 58.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 58/58 [00:15<00:00,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6558, Train Acc: 59.79%, Val Loss: 0.6688, Val Acc: 58.85%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 58/58 [00:15<00:00,  3.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6577, Train Acc: 59.52%, Val Loss: 0.6743, Val Acc: 57.87%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 58/58 [00:15<00:00,  3.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6616, Train Acc: 60.50%, Val Loss: 0.6907, Val Acc: 56.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 58/58 [00:15<00:00,  3.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6504, Train Acc: 61.76%, Val Loss: 0.6607, Val Acc: 60.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 58/58 [00:15<00:00,  3.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6600, Train Acc: 60.01%, Val Loss: 0.6713, Val Acc: 57.69%\nFeature map size after convolutions: 57600\nFeature map size after convolutions: 57600\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3247713117.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_20x_early.load_state_dict(torch.load('best_patch_cnn.pth'))\n/tmp/ipykernel_31/3247713117.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_5x_early.load_state_dict(torch.load('best_patch_cnn.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Training EM-based models for 5X magnification...\nFeature map size after convolutions: 57600\nFeature map size after convolutions: 57600\nInitial training with all patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 292/292 [01:15<00:00,  3.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6844, Train Acc: 55.87%, Val Loss: 0.6747, Val Acc: 58.81%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 292/292 [01:15<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6811, Train Acc: 57.36%, Val Loss: 0.6803, Val Acc: 56.32%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 292/292 [01:14<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6753, Train Acc: 58.81%, Val Loss: 0.6696, Val Acc: 60.09%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 292/292 [01:14<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6733, Train Acc: 58.82%, Val Loss: 0.6717, Val Acc: 59.41%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 292/292 [01:14<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6729, Train Acc: 59.23%, Val Loss: 0.6772, Val Acc: 57.82%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 292/292 [01:15<00:00,  3.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6861, Train Acc: 55.56%, Val Loss: 0.6797, Val Acc: 58.81%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 292/292 [01:14<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6807, Train Acc: 57.61%, Val Loss: 0.6707, Val Acc: 59.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 292/292 [01:14<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6784, Train Acc: 58.12%, Val Loss: 0.6703, Val Acc: 59.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 292/292 [01:14<00:00,  3.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6733, Train Acc: 58.90%, Val Loss: 0.6729, Val Acc: 59.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 292/292 [01:14<00:00,  3.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6726, Train Acc: 59.16%, Val Loss: 0.6649, Val Acc: 60.01%\n\nEM Iteration 1/3\nE-step: Selecting discriminative patches...\nM-step: Retraining models with discriminative patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 56/56 [00:14<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6645, Train Acc: 60.14%, Val Loss: 0.6683, Val Acc: 59.62%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 56/56 [00:14<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6659, Train Acc: 60.76%, Val Loss: 0.6619, Val Acc: 60.65%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 56/56 [00:14<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6640, Train Acc: 60.53%, Val Loss: 0.6661, Val Acc: 60.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 56/56 [00:14<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6601, Train Acc: 61.38%, Val Loss: 0.6764, Val Acc: 59.37%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 56/56 [00:14<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6581, Train Acc: 61.21%, Val Loss: 0.6616, Val Acc: 60.52%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 56/56 [00:14<00:00,  3.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6658, Train Acc: 60.93%, Val Loss: 0.6677, Val Acc: 60.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 56/56 [00:14<00:00,  3.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6667, Train Acc: 59.17%, Val Loss: 0.6792, Val Acc: 57.35%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 56/56 [00:15<00:00,  3.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6697, Train Acc: 60.25%, Val Loss: 0.6746, Val Acc: 58.59%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 56/56 [00:14<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6594, Train Acc: 61.72%, Val Loss: 0.6657, Val Acc: 60.61%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 56/56 [00:14<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6671, Train Acc: 60.42%, Val Loss: 0.6643, Val Acc: 60.61%\n\nEM Iteration 2/3\nE-step: Selecting discriminative patches...\nM-step: Retraining models with discriminative patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 56/56 [00:14<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6574, Train Acc: 60.90%, Val Loss: 0.6670, Val Acc: 58.98%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 56/56 [00:14<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6680, Train Acc: 60.22%, Val Loss: 0.6873, Val Acc: 57.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 56/56 [00:14<00:00,  3.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6649, Train Acc: 59.61%, Val Loss: 0.6659, Val Acc: 60.14%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 56/56 [00:14<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6605, Train Acc: 61.29%, Val Loss: 0.6685, Val Acc: 59.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 56/56 [00:14<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6571, Train Acc: 61.12%, Val Loss: 0.6755, Val Acc: 59.28%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 56/56 [00:14<00:00,  3.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6613, Train Acc: 61.46%, Val Loss: 0.6599, Val Acc: 61.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 56/56 [00:14<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6640, Train Acc: 61.24%, Val Loss: 0.6672, Val Acc: 59.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 56/56 [00:14<00:00,  3.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6636, Train Acc: 61.07%, Val Loss: 0.6609, Val Acc: 60.39%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 56/56 [00:14<00:00,  3.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6625, Train Acc: 60.79%, Val Loss: 0.6674, Val Acc: 59.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 56/56 [00:14<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6578, Train Acc: 62.08%, Val Loss: 0.6550, Val Acc: 61.34%\n\nEM Iteration 3/3\nE-step: Selecting discriminative patches...\nM-step: Retraining models with discriminative patches...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 56/56 [00:14<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6633, Train Acc: 60.36%, Val Loss: 0.6680, Val Acc: 59.84%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 56/56 [00:14<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6626, Train Acc: 60.48%, Val Loss: 0.6668, Val Acc: 59.11%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 56/56 [00:14<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6607, Train Acc: 60.65%, Val Loss: 0.6707, Val Acc: 59.45%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 56/56 [00:14<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6661, Train Acc: 60.48%, Val Loss: 0.6720, Val Acc: 58.89%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 56/56 [00:14<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6607, Train Acc: 61.16%, Val Loss: 0.6668, Val Acc: 59.84%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 56/56 [00:14<00:00,  3.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Train Loss: 0.6603, Train Acc: 60.99%, Val Loss: 0.6603, Val Acc: 60.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 56/56 [00:14<00:00,  3.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Train Loss: 0.6675, Train Acc: 60.36%, Val Loss: 0.6767, Val Acc: 57.61%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 56/56 [00:14<00:00,  3.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Train Loss: 0.6660, Train Acc: 59.97%, Val Loss: 0.6648, Val Acc: 60.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 56/56 [00:14<00:00,  3.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Train Loss: 0.6659, Train Acc: 61.38%, Val Loss: 0.6665, Val Acc: 59.41%\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 56/56 [00:14<00:00,  3.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Train Loss: 0.6641, Train Acc: 59.85%, Val Loss: 0.6610, Val Acc: 61.17%\nFeature map size after convolutions: 57600\nFeature map size after convolutions: 57600\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3247713117.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_20x_early.load_state_dict(torch.load('best_patch_cnn.pth'))\n/tmp/ipykernel_31/3247713117.py:122: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model_5x_early.load_state_dict(torch.load('best_patch_cnn.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Training decision fusion model...\nCreating histograms for training...\nFitting decision fusion model...\nEvaluating on validation set...\nValidation accuracy: 0.5825\nLoading patches for 85 patients...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 85/85 [00:29<00:00,  2.90it/s]","output_type":"stream"},{"name":"stdout","text":"Evaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Test accuracy: 0.5172\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}